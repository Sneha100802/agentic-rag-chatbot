# -*- coding: utf-8 -*-
"""llm_response_agent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t2NGMxwo_JHtKz9TH5EYzg8kDd-LdZ_u

# Explanation :


*  Uses a local Hugging Face model (flan-t5-base) to generate answers.

*  No OpenAI key or cloud API required.
"""

from transformers import pipeline
from utils.mcp import create_mcp_message

class LLMResponseAgent:
    def __init__(self):
        self.qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base")

    def generate_answer(self, context_chunks, query):
        context = "\n".join(context_chunks)
        prompt = f"Answer the question based on context:\nContext: {context}\nQuestion: {query}\nAnswer:"
        result = self.qa_pipeline(prompt, max_new_tokens=200)[0]['generated_text']
        return create_mcp_message(
            sender="LLMResponseAgent",
            receiver="UI",
            msg_type="FINAL_ANSWER",
            payload={"answer": result.strip(), "sources": context_chunks}
        )